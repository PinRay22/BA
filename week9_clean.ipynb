{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import json, copy, os\n",
    "\n",
    "in_path  = \"/content/week9.ipynb\"         # ← 你目前的檔名路徑\n",
    "out_path = \"/content/week9_clean.ipynb\"   # ← 乾淨版\n",
    "\n",
    "with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# 1) 移除 notebook 級 widgets metadata\n",
    "nb.get(\"metadata\", {}).pop(\"widgets\", None)\n",
    "\n",
    "# 2) 清除每個 cell 的 widgets 相關與輸出\n",
    "for cell in nb.get(\"cells\", []):\n",
    "    if isinstance(cell.get(\"metadata\"), dict):\n",
    "        cell[\"metadata\"].pop(\"widgets\", None)\n",
    "    # 可選：也清掉輸出，避免頁面肥大與相容性問題\n",
    "    if \"outputs\" in cell:\n",
    "        cell[\"outputs\"] = []\n",
    "    if \"execution_count\" in cell:\n",
    "        cell[\"execution_count\"] = None\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nb, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "print(\"Cleaned notebook saved to:\", out_path)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "CXmj5H2Ch-E7",
    "outputId": "385b0354-8120-4798-c9bd-9f9f8747da25"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "set -euxo pipefail\n",
    "\n",
    "python -V\n",
    "pip -V\n",
    "\n",
    "# 清掉會衝突或你不需要的套件\n",
    "pip uninstall -y bitsandbytes triton cudf-cu12 pylibcudf-cu12 dask-cudf-cu12 rmm-cu12 || true\n",
    "\n",
    "# 基本工具\n",
    "pip install -U pip setuptools wheel --quiet\n",
    "\n",
    "# RAPIDS 常見相依：固定 pyarrow < 20，pandas < 2.2\n",
    "pip install -U \"pyarrow<20\" \"pandas<2.2\" --quiet --no-warn-conflicts\n",
    "\n",
    "# 安裝與本作業相容的版本（不含 bitsandbytes）\n",
    "pip install -U \\\n",
    "  \"transformers==4.45.2\" \\\n",
    "  \"datasets==2.20.0\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"peft==0.13.2\" \\\n",
    "  \"evaluate==0.4.2\" \\\n",
    "  \"scikit-learn==1.4.2\" \\\n",
    "  \"matplotlib==3.8.4\" \\\n",
    "  --quiet --no-warn-conflicts\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGLkN09Lm-e3",
    "outputId": "251e4f61-8f77-4935-edd1-fe9bf70c9dc8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)\n"
   ],
   "metadata": {
    "id": "wo8E8QiKpbXW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ========= Fast Dev Run：極速小訓練，驗證整體流程 =========\n",
    "import os, math, random, json, numpy as np, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "SEED=42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ==== 極速參數（可之後放大）====\n",
    "MODEL_NAME   = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # 更穩可用 \"Qwen/Qwen2-0.5B\"\n",
    "MAX_LENGTH   = 96\n",
    "BATCH_TRAIN  = 2\n",
    "BATCH_EVAL   = 8\n",
    "GRAD_ACCUM   = 1\n",
    "EPOCHS       = 1\n",
    "LR           = 2e-4\n",
    "OUTPUT_DIR   = \"/content/devrun_lora\"\n",
    "N_TRAIN, N_VAL, N_TEST = 500, 200, 200   # <<< 小資料量\n",
    "\n",
    "# ==== Emotion → 3 類風險 ====\n",
    "raw_ds = load_dataset(\"emotion\")\n",
    "label_names = raw_ds[\"train\"].features[\"label\"].names\n",
    "name_to_risk = {\"joy\":0,\"love\":0,\"surprise\":0,\"anger\":1,\"fear\":1,\"sadness\":2}\n",
    "id2label = {0:\"low_risk\",1:\"mid_risk\",2:\"high_risk\"}\n",
    "\n",
    "def to_risk(ex):\n",
    "    ex[\"risk_label\"] = name_to_risk[label_names[ex[\"label\"]]]\n",
    "    return ex\n",
    "ds = raw_ds.map(to_risk)\n",
    "\n",
    "# 子集（打亂後取前 N）\n",
    "def take_small(split, n):\n",
    "    d = ds[split].shuffle(seed=SEED)\n",
    "    if n is not None and n < len(d):\n",
    "        d = d.select(range(n))\n",
    "    return d\n",
    "\n",
    "small_ds = {\n",
    "    \"train\": take_small(\"train\", N_TRAIN),\n",
    "    \"validation\": take_small(\"validation\", N_VAL),\n",
    "    \"test\": take_small(\"test\", N_TEST),\n",
    "}\n",
    "\n",
    "# ==== Tokenizer（補 pad）====\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.add_special_tokens({\"pad_token\": tok.eos_token})\n",
    "\n",
    "def tok_fn(b):\n",
    "    return tok(b[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "tok_ds = DatasetDict({\n",
    "    \"train\": small_ds[\"train\"].map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"]),\n",
    "    \"validation\": small_ds[\"validation\"].map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"]),\n",
    "    \"test\": small_ds[\"test\"].map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"]),\n",
    "})\n",
    "# 對齊 HF 預設標籤欄位名稱\n",
    "tok_ds = tok_ds.rename_column(\"risk_label\", \"labels\")\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok, pad_to_multiple_of=8)\n",
    "\n",
    "# ==== Base 模型 + LoRA（無 bitsandbytes）====\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "# 同步 pad 與 embedding 長度\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "base.config.use_cache = False\n",
    "if hasattr(base, \"gradient_checkpointing_enable\"):\n",
    "    base.gradient_checkpointing_enable()\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # 省顯存版本\n",
    "    bias=\"none\", task_type=\"SEQ_CLS\",\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "print(\"Trainable params:\"); model.print_trainable_parameters()\n",
    "\n",
    "# ==== 指標 ====\n",
    "def metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    out = {\n",
    "        \"f1_macro\":    f1_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "    try: out[\"auroc_macro\"] = roc_auc_score(labels, probs, multi_class=\"ovr\", average=\"macro\")\n",
    "    except: out[\"auroc_macro\"] = float(\"nan\")\n",
    "    try: out[\"pr_auc_macro\"] = average_precision_score(np.eye(3)[labels], probs, average=\"macro\")\n",
    "    except: out[\"pr_auc_macro\"] = float(\"nan\")\n",
    "    return out\n",
    "\n",
    "# ==== 訓練設定（快速）====\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"steps\",       # 每隔幾步就驗證一次，更快看到數字\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"no\",          # 不存 checkpoint，加速\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    max_steps=300,               # <<< 強制只跑 300 steps（超快）\n",
    "    learning_rate=LR,\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=False,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=tok_ds[\"train\"],\n",
    "    eval_dataset=tok_ds[\"validation\"],\n",
    "    tokenizer=tok, data_collator=collator,\n",
    "    compute_metrics=metrics\n",
    ")\n",
    "\n",
    "# ==== Train / Eval（小跑）====\n",
    "trainer.train()\n",
    "val_metrics = trainer.evaluate(tok_ds[\"validation\"])\n",
    "print(\"Validation (small run):\", json.dumps(val_metrics, indent=2))\n",
    "\n",
    "# ==== Test（小跑）====\n",
    "pred = trainer.predict(tok_ds[\"test\"])\n",
    "logits = pred.predictions\n",
    "probs  = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "y_true = np.array(tok_ds[\"test\"][\"labels\"])     # <<< 修正：用 labels\n",
    "y_pred = probs.argmax(axis=-1)\n",
    "\n",
    "res = {\n",
    "    \"f1_macro\":    f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "}\n",
    "try: res[\"auroc_macro\"] = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n",
    "except: res[\"auroc_macro\"] = float(\"nan\")\n",
    "try: res[\"pr_auc_macro\"] = average_precision_score(np.eye(3)[y_true], probs, average=\"macro\")\n",
    "except: res[\"pr_auc_macro\"] = float(\"nan\")\n",
    "print(\"Test (small run):\", json.dumps(res, indent=2))\n",
    "\n",
    "print(\"\\n=== Classification Report (Test, small) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(3)], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# ==== 小型可視化 ====\n",
    "high = probs[:,2]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(high); plt.grid(True)\n",
    "plt.title(\"High-Risk Probability Trend (small run)\"); plt.xlabel(\"Index\"); plt.ylabel(\"P(high)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "win = max(20, len(high)//10)  # 視窗依樣本量調小\n",
    "win_means = [high[i*win:(i+1)*win].mean() for i in range((len(high)+win-1)//win)]\n",
    "heat = np.array(win_means)[None,:]\n",
    "plt.figure(figsize=(10,1.6))\n",
    "plt.imshow(heat, aspect=\"auto\", cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Avg P(high)\")\n",
    "plt.yticks([]); plt.xticks(range(heat.shape[1]), [f\"W{i+1}\" for i in range(heat.shape[1])])\n",
    "plt.title(f\"High-Risk Intensity Heatmap (window={win})\")\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d6cfede4a36f49b880efed45f2252044",
      "0465d6c6e3c24849bc08de1264de5756",
      "c566907fa72c47a99372d0ec5964e279",
      "8d9c6b1ead874f5b8c394200685b4d6f",
      "c5405e3ebaed4aab8d5f725f6c9d0271",
      "bcba629dbb774865b93c4cc34fe42eb8",
      "33cd4db4c4de4e239ba948a8a88feb9c",
      "4756a4e5aacd48448a3fe672f488db54",
      "cad7b9a96541476f981a58d55ca735c6",
      "2488e8e09bf04dc6bffb57a52a67bd0a",
      "fa9cfc3fc1fe480d9f98e36ae65684e1",
      "b07420c985e04830a80da47ca92d5781",
      "c35ace8f7d3a4e2685f4b820db3c1ab6",
      "347791cab5504736b31f5668f30f419c",
      "ef2a947cb95346b68b6c561cb6662fff",
      "c6c76c2fa6914d1da73531161324b58b",
      "5a8ecd07ac50427bb10e0aae5978bbe6",
      "e89fec35906f4aa4aaaa8a4d7aa2b911",
      "8af0b2d757ae4f48b6ddc77c48315cb1",
      "123ea3e859dc4a30a754aa6073655396",
      "e34a9ad921014959918f2d9ddd406787",
      "4017e81727cb4cd7900c1d0df048b973",
      "647bdbb86284431294ea0ffd741980d0",
      "73a8c5a722ba44048d32693948510f04",
      "237e7e052e6043a4b8c6ca8987b5b8c5",
      "ed420f3459da4164b36a3235042ea219",
      "bf4195b951fa451d8d2489456eff0296",
      "3cbc2d65ad314bcabead36db366eb6a7",
      "f217a0cf316d4fb28849a75fe246cadb",
      "a7ca82df351f45d9a496c6c515bed5e6",
      "c2d7512e6bc3428bb9b3cac27431e5eb",
      "cbdece1e3cf443d781f46379a1f72b83",
      "150e39eb181148338948404dfe8473be",
      "4ce2eda1f9a2418fba1419200139e0ff",
      "81087d7a5747472891592971a628ed32",
      "c79be896fa784ab492659c3c80e294d1",
      "0181a2b49013425892b2b5493ed05c85",
      "208fd6dcf2f64d5ab786cdc6727c250f",
      "731f7cbd22464f84a637db382a7c41e8",
      "1ab104a813a8416f91016b6b1807baa8",
      "6b68c5fcda554f49bb6855bef32dda84",
      "41dae9bcf4f24e90b00eba50a8eb3da2",
      "b1a25fc928af44a9801cb09fbd34eb20",
      "a4aa04822aa44c9792788f1e69d4c83a",
      "779348b4e02a419384ca6c9a4b4829a0",
      "f793eb49dab641e09b0b3cd7cbb1cd0d",
      "8d5148c5432b474c9b12e74655af3092",
      "5b221bc98d3d4c1383a65481c168fa00",
      "d19cab391e7d4296bbf816377736d8e7",
      "d159e3055b654bb683013e7a9d3c5991",
      "0a9da803c4c14ae7a08398742b0b1e4c",
      "ca78b79b4e9c457293f74f0be6e3bc62",
      "78b105d7bee94596b3e259ec38d0c1f3",
      "19cc2d0652e948068bfed5dcf8dc8963",
      "41d8f1d3154e488fb4bffe37f2ea036f",
      "799941bc1b5f44869b707c69680f9c51",
      "47130093e7494b8c974eda26a10b27d2",
      "b7426d8d1f86468e8c2836cdae5e0d89",
      "eab5d85e93984b069e52d9d53023d47f",
      "111239affafa4403902c224b71b41e02",
      "b228a3eea8284b0abfa8ee37c4e12276",
      "1c7b4976fbab49fb8a3f2e6d0dd547e5",
      "9c98e6ddebb14dfe882d32b6f421d1ff",
      "37997d6eac95484ba0478fe085498ca4",
      "75a9c72e820240e9b17b22665cd3fb18",
      "dab551d59d4948fcaf9b6b444f817175"
     ]
    },
    "id": "Q3SAOVaj357p",
    "outputId": "90b44e04-0b38-4f15-efd2-b6cb3d98bd88"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c55008a4b219415b9b2bda660dbfb5c7",
      "bd2ebdb596754681bc25153c33d90350",
      "ccd9e7958f664a35a8de7c38e67a143a",
      "f0f49ac520524675843e157c499e01c7",
      "56d543b6c16d4d7fa03d7a56c1cfe911",
      "f8df1173a6f242c68654dd946bec0f03",
      "2825d1884f184b8bad33e74bf9564dd8",
      "481b31f3a823441e8ad02dc2964680a6",
      "399dcad27e324b32ada70040577466d9",
      "036fe521c00b4246bb8d565b18d1eb8d",
      "66bd7afd082a4a0c866c11543d03a63d",
      "e73f0c89251f40b0ba456c1961429641",
      "9845a12fda2c45d2a2b36c4ffa7b93e4",
      "a6873d10eb074d93b5d09420132b26f8",
      "37cf9f9f48b44d2bb24f0084e3e54f02",
      "2991b164f08147b78265f52e0809c1b8",
      "0f691c608f9542698265356a909511a3",
      "d1deae09c6e8443ba35486d2a39fe924",
      "642dc4d371ab4c21bbe6c7c56b3ccffb",
      "01bb0524830343ddbe168448675babcb",
      "ceb5e258308346a6a13817678c77b939",
      "4089052b514f4fdb9e41dbf01c2f1160",
      "1009990b94934526a327a584f82d4a26",
      "0dbb413467cb473093f9d51007c8b3c2",
      "97eb3c3a45db4210a72d8ca413434a80",
      "b5e8fa0e59c54503aab8e88d38ef0963",
      "c6eec4aa03fa458db6eb93174dc93c7a",
      "ddfea72f407749009a26752e45f517c8",
      "ff8237acc9cc496d9316288f8b29b217",
      "289da159adea4c82ae7356b0fcc45bae",
      "84edf51dd3a74a92971652a6d3142cb2",
      "06959b1452d34311a09e3fbb8fc8afb4",
      "34973505876e4266bca0ba26bc4da055",
      "f5590d7758894e528e571f4a9c5a8f0e",
      "276bbfdaccfa4c7588aae40f9a991324",
      "e0f75233263047dca394875580df175f",
      "a5f4fcaa292541f2a8d7d9f08971497b",
      "0dc10101822f49e38ba3a648fdd08fd3",
      "16a457717ea4466f8ccd888ad58c0056",
      "ae850608bd374e938c0cc9c2acf53a37",
      "99295bedb8234db9a5909206b80146ac",
      "6dbd266820f043efa148b5ee2051c7cb",
      "28910fa1bfbc4fa7a51e2ac02f7669b0",
      "02be842aa18f484e8752d61097735c8c",
      "053caf18d86f4365975d85fd88bd3c54",
      "2861719bf5c3448fa7d408f6a7e89ab7",
      "a82b9ca9a7af4be08c006fdc04bdc217",
      "c2f054c98cda457d98b1ba0ccfc7aca5",
      "3573456b2fb047ecaa0fa7b0766769d0",
      "9bf630adeca44cf7a1b6cfe2d61eedcc",
      "e07b5752799e4278b231a4f430d15ff9",
      "da08583528434278a7daa52b9eb1c9f4",
      "e228ad1b07584fd99decb06741e1b1e9",
      "782cca2673474da588f098a6bdf4837a",
      "840f7c4dbc174611985313a13d168ec8",
      "db59aa5772a34005bbf7a81b78326272",
      "b289966da2ed465694c23eda4667c405",
      "401268a153b2436f829b165a6a97bae5",
      "f1cf3a1bbae242788377d561be9f6a17",
      "3c71cc5749264715ac54add973b5acbb",
      "41f42fe98bac40d79fbd3c2978e669ad",
      "44119aecb3934358a12122d9a046633f",
      "0abbeb5a00094a26b0b7194babefff4d",
      "6145b59da0ae4da9ab4da26de7b522a8",
      "69574f25c1724fd1a7d52c1ab4cc7e82",
      "9eea80cd6d3e480da451c27d4b8d26c9"
     ]
    },
    "id": "QDMoBT1plgvG",
    "outputId": "1b0d8b7c-9cab-4158-d6b4-47098f236d92"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# LLM Emotion → Depression Risk (LoRA) | Colab-ready\n",
    "# =========================================================\n",
    "# ✅ 無需 bitsandbytes / triton（不走 QLoRA）\n",
    "# ✅ 內建類別權重 + EarlyStopping + 線性學習率排程\n",
    "# ✅ 一鍵快速小跑 / 正式訓練切換\n",
    "# =========================================================\n",
    "\n",
    "import os, math, random, json, numpy as np, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
    "                             roc_auc_score, average_precision_score)\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------\n",
    "# 環境檢查\n",
    "# -----------------------\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    import subprocess; print(subprocess.check_output([\"nvidia-smi\"]).decode())\n",
    "\n",
    "# -----------------------\n",
    "# 全域參數\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ★ 切換快速小跑 / 正式訓練\n",
    "FAST_DEV = True   # True=小跑；False=正式訓練\n",
    "\n",
    "# 底模（想更穩、更省顯存，可改成 \"Qwen/Qwen2-0.5B\"）\n",
    "MODEL_NAME   = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MAX_LENGTH   = 96 if FAST_DEV else 120\n",
    "BATCH_TRAIN  = 4\n",
    "BATCH_EVAL   = 8\n",
    "GRAD_ACCUM   = 4\n",
    "EPOCHS       = 1 if FAST_DEV else 2\n",
    "LR           = 2e-4\n",
    "OUTPUT_DIR   = \"/content/llm_emotion_risk_lora_final\"\n",
    "\n",
    "# FAST_DEV 的子集大小與訓練步數限制\n",
    "if FAST_DEV:\n",
    "    N_TRAIN, N_VAL, N_TEST = 2000, 800, 800\n",
    "    MAX_STEPS = 600     # 小跑更快看到效果\n",
    "else:\n",
    "    N_TRAIN, N_VAL, N_TEST = None, None, None\n",
    "    MAX_STEPS = -1      # 使用完整 epoch\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# 資料：Emotion → 3 類風險（低/中/高）\n",
    "# -----------------------\n",
    "raw_ds = load_dataset(\"emotion\")\n",
    "label_names = raw_ds[\"train\"].features[\"label\"].names\n",
    "name_to_risk = {\"joy\":0, \"love\":0, \"surprise\":0, \"anger\":1, \"fear\":1, \"sadness\":2}\n",
    "id2label = {0: \"low_risk\", 1: \"mid_risk\", 2: \"high_risk\"}\n",
    "\n",
    "def to_risk(ex):\n",
    "    ex[\"risk_label\"] = name_to_risk[label_names[ex[\"label\"]]]\n",
    "    return ex\n",
    "\n",
    "ds = raw_ds.map(to_risk)\n",
    "\n",
    "# 取小子集（FAST_DEV）\n",
    "def take_small(split_name, n):\n",
    "    d = ds[split_name].shuffle(seed=SEED)\n",
    "    if n is not None and n < len(d):\n",
    "        d = d.select(range(n))\n",
    "    return d\n",
    "\n",
    "if FAST_DEV:\n",
    "    ds_small = DatasetDict({\n",
    "        \"train\":      take_small(\"train\", N_TRAIN),\n",
    "        \"validation\": take_small(\"validation\", N_VAL),\n",
    "        \"test\":       take_small(\"test\", N_TEST),\n",
    "    })\n",
    "else:\n",
    "    ds_small = ds\n",
    "\n",
    "# -----------------------\n",
    "# Tokenizer（補 pad_token）\n",
    "# -----------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.add_special_tokens({\"pad_token\": tok.eos_token})\n",
    "\n",
    "def tok_fn(b):\n",
    "    return tok(b[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "\n",
    "tok_ds = DatasetDict({\n",
    "    \"train\":      ds_small[\"train\"].map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"]),\n",
    "    \"validation\": ds_small[\"validation\"].map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"]),\n",
    "    \"test\":       ds_small[\"test\"].map(tok_fn, batched=True, remove_columns=[\"text\",\"label\"]),\n",
    "})\n",
    "tok_ds = tok_ds.rename_column(\"risk_label\", \"labels\")  # 與 HF 預設對齊\n",
    "collator = DataCollatorWithPadding(tokenizer=tok, pad_to_multiple_of=8)\n",
    "\n",
    "# -----------------------\n",
    "# Base 模型（LoRA，無 bitsandbytes）\n",
    "# -----------------------\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# 同步 embedding 與 pad\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "base.config.use_cache = False\n",
    "if hasattr(base, \"gradient_checkpointing_enable\"):\n",
    "    base.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA：省顯存但更有效（q,v,o）\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"o_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "print(\"Trainable params:\"); model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------\n",
    "# 指標\n",
    "# -----------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "    out = {\n",
    "        \"f1_macro\":    f1_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "    try: out[\"auroc_macro\"] = roc_auc_score(labels, probs, multi_class=\"ovr\", average=\"macro\")\n",
    "    except: out[\"auroc_macro\"] = float(\"nan\")\n",
    "    try: out[\"pr_auc_macro\"] = average_precision_score(np.eye(3)[labels], probs, average=\"macro\")\n",
    "    except: out[\"pr_auc_macro\"] = float(\"nan\")\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# 類別權重（緩解中/高風險不平衡）\n",
    "# -----------------------\n",
    "train_labels = np.array(tok_ds[\"train\"][\"labels\"])\n",
    "cnt = Counter(train_labels.tolist())\n",
    "num_classes = 3\n",
    "freq = np.array([cnt.get(i, 0) for i in range(num_classes)], dtype=float)\n",
    "class_weights = torch.tensor((freq.sum() / (freq + 1e-9)) / num_classes,\n",
    "                             dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n",
    "print(\"Class weights:\", class_weights.tolist())\n",
    "\n",
    "class WeightedCETrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        w = class_weights.to(logits.device).to(logits.dtype)\n",
    "        loss = nn.functional.cross_entropy(logits, labels, weight=w)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------\n",
    "# 訓練設定（含早停與線性 LR）\n",
    "# -----------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # ⚠️ 新版警告：evaluation_strategy 將被棄用，這裡先換成 eval_strategy 以免再跳警告\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "\n",
    "    # ✅ 早停需要保存檢查點，並在最後載入最佳模型\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    fp16=False,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    max_steps=(None if MAX_STEPS == -1 else MAX_STEPS),  # 有設定時會覆蓋 epochs → 正常\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds[\"train\"],\n",
    "    eval_dataset=tok_ds[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
    "\n",
    "# -----------------------\n",
    "# 訓練 / 驗證\n",
    "# -----------------------\n",
    "train_out = trainer.train()\n",
    "val_metrics = trainer.evaluate(tok_ds[\"validation\"])\n",
    "print(\"Validation:\", json.dumps(val_metrics, indent=2))\n",
    "\n",
    "# -----------------------\n",
    "# 測試\n",
    "# -----------------------\n",
    "pred = trainer.predict(tok_ds[\"test\"])\n",
    "logits = pred.predictions\n",
    "probs  = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "y_true = np.array(tok_ds[\"test\"][\"labels\"])\n",
    "y_pred = probs.argmax(axis=-1)\n",
    "\n",
    "res = {\n",
    "    \"f1_macro\":    f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "}\n",
    "try: res[\"auroc_macro\"] = roc_auc_score(y_true, probs, multi_class=\"ovr\", average=\"macro\")\n",
    "except: res[\"auroc_macro\"] = float(\"nan\")\n",
    "try: res[\"pr_auc_macro\"] = average_precision_score(np.eye(3)[y_true], probs, average=\"macro\")\n",
    "except: res[\"pr_auc_macro\"] = float(\"nan\")\n",
    "print(\"Test:\", json.dumps(res, indent=2))\n",
    "\n",
    "print(\"\\n=== Classification Report (Test) ===\")\n",
    "print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(3)], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# -----------------------\n",
    "# 可視化\n",
    "# -----------------------\n",
    "high = probs[:,2]\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(high)\n",
    "plt.title(\"High-Risk Probability Trend\" + (\" (FAST_DEV)\" if FAST_DEV else \"\"))\n",
    "plt.xlabel(\"Index\"); plt.ylabel(\"P(high)\"); plt.grid(True)\n",
    "\n",
    "win = max(20, len(high)//10)\n",
    "win_means = [high[i*win:(i+1)*win].mean() for i in range((len(high)+win-1)//win)]\n",
    "heat = np.array(win_means)[None,:]\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(heat, aspect=\"auto\", cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Avg P(high)\")\n",
    "plt.yticks([]); plt.xticks(range(heat.shape[1]), [f\"W{i+1}\" for i in range(heat.shape[1])])\n",
    "plt.title(f\"High-Risk Intensity Heatmap (window={win})\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# 錯誤分析（前 10 筆）\n",
    "# -----------------------\n",
    "texts = (ds_small if FAST_DEV else ds)[\"test\"][\"text\"]\n",
    "mis = np.where(y_pred != y_true)[0][:10]\n",
    "print(\"\\n--- Error Analysis (first 10) ---\")\n",
    "for i, idx in enumerate(mis, 1):\n",
    "    print(f\"[{i}] idx={idx}\")\n",
    "    print(\"Text:\", texts[idx][:300].replace(\"\\n\",\" \"))\n",
    "    print(f\"True={id2label[int(y_true[idx])]} Pred={id2label[int(y_pred[idx])]} p={probs[idx].round(3)}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# -----------------------\n",
    "# 匯出\n",
    "# -----------------------\n",
    "np.save(os.path.join(OUTPUT_DIR, \"test_cm.npy\"), cm)\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_metrics.json\"), \"w\") as f:\n",
    "    json.dump(res, f, indent=2)\n",
    "from pathlib import Path\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "dwLPH4i1nIh5"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}